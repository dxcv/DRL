{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DRL_PMP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xrvChP937jOL","colab_type":"text"},"source":["# Deer Reinforcement Learning - Portfolio Management Problem\n"]},{"cell_type":"markdown","metadata":{"id":"qtQb5B2y7fbu","colab_type":"text"},"source":["### Imports and requirements\n"]},{"cell_type":"code","metadata":{"id":"340N3l0q7gk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"77ca9642-9a99-41d1-c6b2-c5031186515e","executionInfo":{"status":"ok","timestamp":1577567493975,"user_tz":-60,"elapsed":23490,"user":{"displayName":"Jakub Polak","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNMDVLVeead_7VqHspJSIk8HuMLGwg_8aQ6ka0Zw=s64","userId":"09079568061074341961"}}},"source":["# Mount Drive and set directory\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/DRL\n","!ls "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/DRL\n","Data  DRL_PMP.ipynb  input.npy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OCZLxgQHoQqT","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import math\n","import gym\n","\n","import tensorflow as tf\n","import numpy as np\n","from collections import deque\n","import random\n","import pandas as pd\n","#import ffn\n","\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wrTiTaxzOyL6","colab_type":"text"},"source":["### Parameters Setup\n"]},{"cell_type":"code","metadata":{"id":"km5iR7Z9oZDP","colab_type":"code","colab":{}},"source":["data_dir = 'StocksD' #@param ['StocksD', 'StocksH', 'StocksM', 'CryptoH']\n","\n","path_data = './input.npy'\n","data_dir = 'StocksD'\n","data_type = data_dir\n","stocks_tickers = ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO'] \n","list_stock = stocks_tickers\n","data = np.load(path_data)\n","trading_period = data.shape[2]\n","nb_feature_map = data.shape[0]\n","nb_stocks = data.shape[1]\n","m = nb_stocks\n","f = nb_feature_map\n","t = trading_period\n","\n","###############################dictionaries of the problem###########################\n","dict_hp_net = {'n_filter_1': 2, 'n_filter_2': 20, 'kernel1_size':(1, 3)}\n","dict_hp_pb = {'batch_size': 50, 'ratio_train': 0.6,'ratio_val': 0.2, 'length_tensor': 10,\n","              'ratio_greedy':0.8, 'ratio_regul': 0.1}\n","dict_hp_opt = {'regularization': 1e-8, 'learning': 9e-2}\n","dict_fin = {'trading_cost': 0.25/100, 'interest_rate': 0.02/250, 'cash_bias_init': 0.7}\n","dict_train = {'pf_init_train': 10000, 'w_init_train': 'd', 'n_episodes':2, 'n_batches':10}\n","dict_test = {'pf_init_test': 10000, 'w_init_test': 'd'}\n","###############################HP of the network ###########################\n","n_filter_1 = dict_hp_net['n_filter_1'] # 2\n","n_filter_2 = dict_hp_net['n_filter_2'] # 20\n","kernel1_size = dict_hp_net['kernel1_size'] #(1,3)\n","###############################HP of the problem###########################\n","batch_size = dict_hp_pb['batch_size']                               # Size of mini-batch during training\n","total_steps_train = int(dict_hp_pb['ratio_train']*trading_period)   # Total number of steps for pre-training in the training set\n","total_steps_val = int(dict_hp_pb['ratio_val']*trading_period)       # Total number of steps for pre-training in the validation set\n","total_steps_test = trading_period-total_steps_train-total_steps_val # Total number of steps for the test\n","n = dict_hp_pb['length_tensor']                                     # Num of columns (number of the trading periods) in each input price matrix\n","ratio_greedy = dict_hp_pb['ratio_greedy']\n","ratio_regul = dict_hp_pb['ratio_regul']\n","##############################HP of the optimization###########################\n","regularization = dict_hp_opt['regularization']                      # The L2 regularization coefficient applied to network training\n","learning = dict_hp_opt['learning']                                  # Parameter alpha (i.e. the step size) of the Adam optimization\n","optimizer = tf.train.AdamOptimizer(learning)\n","##############################Finance parameters###########################\n","trading_cost= dict_fin['trading_cost']\n","interest_rate= dict_fin['interest_rate']\n","cash_bias_init = dict_fin['cash_bias_init']\n","sample_bias = 5e-5                                                  # Beta in the geometric distribution for online training sample batches\n","############################## Training Parameters ###########################\n","w_init_train = np.array(np.array([1]+[0]*m))    #dict_train['w_init_train']\n","pf_init_train = dict_train['pf_init_train']\n","n_episodes = dict_train['n_episodes']\n","n_batches = dict_train['n_batches']\n","############################## Test Parameters ###########################\n","w_init_test = np.array(np.array([1]+[0]*m))     #dict_test['w_init_test']\n","pf_init_test = dict_test['pf_init_test']\n","############################## other environment Parameters ###########################\n","w_eq = np.array(np.array([1/(m+1)]*(m+1)))\n","w_s = np.array(np.array([1]+[0.0]*m))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdotNlRk72e5","colab_type":"text"},"source":["### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"Tedjb-TCoW4D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"6699dd3a-5158-4868-8554-642877d23d1a","executionInfo":{"status":"ok","timestamp":1577567602293,"user_tz":-60,"elapsed":2755,"user":{"displayName":"Jakub Polak","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNMDVLVeead_7VqHspJSIk8HuMLGwg_8aQ6ka0Zw=s64","userId":"09079568061074341961"}}},"source":["directory = os.getcwd() + '/Data/' + data_dir + '/'                             # Specify Directory of Data\n","stock_files = os.listdir(directory)                                             # Get list of all files\n","stock_files.sort()\n","\n","for file in stock_files:                                                        # Remove hidden and unwanted files\n","    if file[0] == '.': stock_files.remove(file)\n","    if file[0] == '~': stock_files.remove(file)\n","\n","selection = []                                                                  # Load only subset of assets\n","if selection: stock_files = [stock_files[i] for i in selection ]\n","\n","stock_tickers = [file.split('_')[0] for file in stock_files]                    # Extract stock names only\n","\n","kept_stocks = list() \n","not_kept_stocks = list()       \n","\n","for s in stock_files:                                                           # Check the history of stocks matches\n","    df = pd.read_csv(directory + s)\n","    if data_dir == 'StocksD':   \n","        input_n = 1259                                                          # Stocks Daily data of 5 years (252*5)\n","        if len(df)>=input_n: kept_stocks.append(s)\n","        else: not_kept_stocks.append(s)\n","    elif data_dir == 'StocksH':\n","        input_n = 1976                                                          # Stocks Half-hour data of 1/2 year (152*6.5*2)\n","        if len(df)>=input_n: kept_stocks.append(s)\n","        else: not_kept_stocks.append(s)\n","    elif data_dir == 'StocksM':\n","        input_n = 59280                                                         # Stocks Minute data of 1/2 year (152*6.5*60)\n","        if len(df)>=input_n: kept_stocks.append(s)\n","        else: not_kept_stocks.append(s)\n","    elif data_dir == 'CryptoH':\n","        input_n = 17520                                                         # Crypto Half-hour data of 1 year (365*24*2)\n","        if len(df)>=input_n: kept_stocks.append(s)\n","        else: not_kept_stocks.append(s)\n","\n","print('\\n There is {} different assets. \\n Following assets will be loaded {}'.format(len(stock_tickers),stock_tickers))\n","if not_kept_stocks: print(' Error in reading following stocks {}'.format(not_kept_stocks))\n","\n","list_open = list()                                                              # Read following features from data\n","list_close = list()\n","list_high = list()\n","list_low = list()\n","\n","for s in tqdm(kept_stocks):                                                     \n","    data = pd.read_csv(directory + s).fillna('bfill').copy()\n","    data = data[['open', 'close', 'high', 'low']]\n","    data = data.tail(input_n)\n","    list_open.append(data.open.values)\n","    list_close.append(data.close.values)\n","    list_high.append(data.high.values)\n","    list_low.append(data.low.values)\n","\n","array_open = np.transpose(np.array(list_open))[:-1]                             # Shift them according to period they belong to\n","array_open_of_the_day = np.transpose(np.array(list_open))[1:]\n","array_close = np.transpose(np.array(list_close))[:-1]\n","array_high = np.transpose(np.array(list_high))[:-1]\n","array_low = np.transpose(np.array(list_low))[:-1]  \n","\n","X = np.transpose(np.array([array_close/array_open,                              # Combine data together into one tensor\n","                           array_high/array_open,\n","                           array_low/array_open,\n","                           array_open_of_the_day/array_open]), axes= (0,2,1))\n","\n","#X = np.transpose(np.array([array_high/array_open,\n","#                           array_low/array_open,\n","#                           array_open_of_the_day/array_open]), axes= (0,2,1))\n","\n","input_f, input_m, input_n = X.shape                                             # Check dimensions of data\n","print('\\n Data have shape {}'.format(X.shape))\n","print(' Number of features: {}'.format(input_f))\n","print(' Number of assets: {}'.format(input_m))\n","print(' Number of samples: {}'.format(input_n))\n","\n","np.save('./input.npy', X)                                                       # Save input tnsor\n","print('\\n Input tensor saved!')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 5/5 [00:00<00:00, 87.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," There is 5 different assets. \n"," Following assets will be loaded ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO']\n","\n"," Data have shape (4, 5, 1258)\n"," Number of features: 4\n"," Number of assets: 5\n"," Number of samples: 1258\n"," Input tensor saved!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"XR_hOiCa76qP","colab_type":"text"},"source":["### Trading Envirionment"]},{"cell_type":"code","metadata":{"id":"TD_6ub0dsvZ3","colab_type":"code","colab":{}},"source":["class TradeEnv():\n","    \"\"\"\n","    This class is the trading environment (render) of our project. \n","    The trading agent calls the class by giving an action at the time t. \n","    Then the render gives back the new portfolio at the next step (time t+1). \n","\n","    # Parameters:\n","    - windonw_length: number of time inputs looked in the past to build the input tensor\n","    - portfolio_value: initial value of the portfolio \n","    - trading_cost: cost (in % of the traded stocks) the agent will pay to execute the action \n","    - interest_rate: rate (in % of the money the agent has) the agent will either get at each step \n","                     if he has a positive amount of money or pay if he has a negative amount of money\n","    -train_size: fraction of data used for the training of the agent, (train -> | time T | -> test)\n","    \"\"\"\n","\n","    def __init__(self, path = './input.npy', window_length=50,\n","                 portfolio_value= 10000, trading_cost= 0.25/100,\n","                 interest_rate= 0.02/250, train_size = 0.7):\n","        \n","        self.path = path                                                        # path to numpy data\n","        self.data = np.load(self.path)                                          # load the input tensor\n","        self.portfolio_value = portfolio_value                                  # initial input value\n","        self.window_length = window_length                                      # window of previous samples\n","        self.trading_cost = trading_cost                                        # trading costs\n","        self.interest_rate = interest_rate                                      # interest rate on money\n","        self.nb_features = self.data.shape[0]                                   # number of features\n","        self.nb_stocks = self.data.shape[1]                                     # number of stocks\n","        self.nb_samples = self.data.shape[2]                                    # number of samples\n","        self.end_train = int((self.nb_samples-self.window_length)*train_size)   # number of training samples\n","        self.index = None                                                       # initial index - integer / time step t currently happening\n","        self.state = None                                                       # initial state - tuple / data, portfolio weights, portfolio value\n","        self.seed()                                                             # initial seed\n","        self.done = False                                                       # epoch indicator\n","\n","    def return_pf(self):\n","        \"\"\"\n","        Current portfolio value\n","        \"\"\"\n","        return self.portfolio_value\n","        \n","    def readTensor(self,X,t): \n","        \"\"\"                                         \n","        Input tensor for NN. All features, All stocks, Current time up to window previous values\n","        \"\"\"\n","        return X[ : , :, t-self.window_length:t ]\n","    \n","    def readUpdate(self, t):\n","        \"\"\"\n","        Return of each stock for the period t \n","        \"\"\"\n","        return np.array([1+self.interest_rate]+self.data[-1,:,t].tolist())\n","\n","    def seed(self, seed=None):\n","        \"\"\"\n","        Set a random seed for reproducibility\n","        \"\"\"\n","        self.np_random, seed = gym.utils.seeding.np_random(seed)\n","        return [seed]\n","    \n","    def reset(self, w_init, p_init, t=0 ):\n","        \"\"\"\n","        Reset the environments' epoch with given first window of data, initial portfolio weights and value of portfolio\n","        \"\"\"\n","        self.state = (self.readTensor(self.data, self.window_length), w_init , p_init )\n","        self.index = self.window_length + t\n","        self.done = False\n","        return self.state, self.done\n","  \n","    def step(self, action):\n","        \"\"\"\n","        Main function of the render. At each step t, the trading agent gives (the action he wants to do) the new value of the weights of the portfolio. \n","        The function computes the new value of the portfolio at the step (t+1), it returns also the reward associated with the action the agent took. \n","        The reward is defined as the evolution of the the value of the portfolio in %. \n","        \"\"\"\n","        index = self.index                                                      # current time step\n","        data = self.readTensor(self.data, index)                                # current input tensor\n","        done = self.done                                                        # current epoch indicator\n","\n","        # Beginning of the day / period\n","        state = self.state                                                      # state space at the beginning of the period\n","        w_previous = state[1]                                                   # weights of the portfolio at the beginning of the day\n","        pf_previous = state[2]                                                  # value of portfolio at the beginning of the day        \n","        update_vector = self.readUpdate(index)                                  # vector of opening price of the period divided by opening price of previous period\n","        w_alloc = action                                                        # action - chosen weights for portfolio for the next step \n","        trans_f = np.linalg.norm((w_alloc-w_previous),ord=1)*self.trading_cost  # transaction remainder factor\n","        cost = pf_previous * trans_f                                            # compute transaction cost\n","        v_alloc = pf_previous * w_alloc                                         # convert weight vector into value vector\n","        v_trans = v_alloc - np.array([cost] + [0]*self.nb_stocks)               # substract the transaction cost from each value vector\n","        \n","        # End of the day / period\n","        v_evol = v_trans*update_vector                                          # compute value evolution of portfolio \n","        pf_evol = np.sum(v_evol)                                                # compute the total new portfolio value\n","        w_evol = v_evol/pf_evol                                                 # compute weight vector of portfolio\n","        reward = (pf_evol-pf_previous)/pf_previous                              # compute instanteanous reward\n","        index = index + 1                                                       # update index\n","        state = (self.readTensor(self.data, index), w_evol, pf_evol)            # compute new state\n","\n","        if index >= self.end_train: done = True                                 # check if epoch has ended\n","        self.state = state                                                      # save state\n","        self.index = index                                                      # save time step\n","        self.done = done                                                        # save epoch indicato\n","        \n","        return state, reward, done\n","        \n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rS0X08PMViWe","colab_type":"text"},"source":["### Deep Policy Network\n"]},{"cell_type":"code","metadata":{"id":"sFKyhmaFtIJm","colab_type":"code","colab":{}},"source":["# define neural net \\pi_\\phi(s) as a class\n","class Policy(object):\n","    '''\n","    This class is used to instanciate the policy network agent\n","    '''\n","\n","    def __init__(self, m, n, sess, optimizer,\n","                 trading_cost=trading_cost,\n","                 interest_rate=interest_rate,\n","                 n_filter_1=n_filter_1,\n","                 n_filter_2=n_filter_2):\n","\n","        # parameters\n","        self.trading_cost = trading_cost\n","        self.interest_rate = interest_rate\n","        self.n_filter_1 = n_filter_1\n","        self.n_filter_2 = n_filter_2\n","        self.n = n\n","        self.m = m\n","\n","        with tf.variable_scope(\"Inputs\"):                                       # Placeholders for variables\n","            self.X_t = tf.placeholder(\n","                tf.float32, [None, nb_feature_map, self.m, self.n])             # The Price tensor\n","            self.W_previous = tf.placeholder(tf.float32, [None, self.m+1])      # weights at the previous time step\n","            self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])      # portfolio value at the previous time step\n","            self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])     # vector of Open(t+1)/Open(t)\n","            #self.pf_value_previous_eq = tf.placeholder(tf.float32, [None, 1])\n","\n","        with tf.variable_scope(\"Policy_Model\"):                                 # Policy function\n","\n","            bias = tf.get_variable('cash_bias', shape=[1, 1, 1, 1], \n","                                   initializer=tf.constant_initializer(cash_bias_init))  # variable of the cash bias\n","            shape_X_t = tf.shape(self.X_t)[0]                                   # shape of the tensor == batchsize\n","            self.cash_bias = tf.tile(bias, tf.stack([shape_X_t, 1, 1, 1]))      # modify to get a \"tensor size\" for the cash bias\n","            # print(self.cash_bias.shape)\n","\n","            with tf.variable_scope(\"Conv1\"):                                    # first layer on the X_t tensor, return a tensor of depth 2\n","                self.conv1 = tf.layers.conv2d(\n","                    inputs=tf.transpose(self.X_t, perm=[0, 3, 2, 1]),\n","                    activation=tf.nn.relu,\n","                    filters=self.n_filter_1,\n","                    strides=(1, 1),\n","                    kernel_size=kernel1_size,\n","                    padding='same')\n","\n","            with tf.variable_scope(\"Conv2\"):                                    # feature maps\n","                self.conv2 = tf.layers.conv2d(\n","                    inputs=self.conv1,\n","                    activation=tf.nn.relu,\n","                    filters=self.n_filter_2,\n","                    strides=(self.n, 1),\n","                    kernel_size=(1, self.n),\n","                    padding='same')\n","\n","            with tf.variable_scope(\"Tensor3\"):                                  # include weights w from last period, modify to have good dimensions\n","                w_wo_c = self.W_previous[:, 1:]\n","                w_wo_c = tf.expand_dims(w_wo_c, 1)\n","                w_wo_c = tf.expand_dims(w_wo_c, -1)\n","                self.tensor3 = tf.concat([self.conv2, w_wo_c], axis=3)\n","\n","            with tf.variable_scope(\"Conv3\"):                                    # last feature map WITHOUT cash bias\n","                self.conv3 = tf.layers.conv2d(\n","                    inputs=self.conv2,\n","                    activation=tf.nn.relu,\n","                    filters=1,\n","                    strides=(self.n_filter_2 + 1, 1),\n","                    kernel_size=(1, 1),\n","                    padding='same')\n","\n","            with tf.variable_scope(\"Tensor4\"):                                  # last feature map WITH cash bias, squeeze to reduce and get the good dimension\n","                self.tensor4 = tf.concat([self.cash_bias, self.conv3], axis=2)  \n","                self.squeezed_tensor4 = tf.squeeze(self.tensor4, [1, 3])       \n","\n","            with tf.variable_scope(\"Policy_Output\"):                            # softmax layer to obtain weights\n","                self.action = tf.nn.softmax(self.squeezed_tensor4)              \n","\n","            with tf.variable_scope(\"Reward\"):                                   # computation of the reward\n","\n","                constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n","                cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n","                y_t = tf.concat([cash_return, self.dailyReturn_t], axis=1)\n","                Vprime_t = self.action * self.pf_value_previous\n","                Vprevious = self.W_previous*self.pf_value_previous\n","\n","                constant = tf.constant(1.0, shape=[1])                          \n","                cost = self.trading_cost * tf.norm(Vprime_t-Vprevious, ord=1, axis=1) * constant\n","                cost = tf.expand_dims(cost, 1)\n","                zero = tf.constant(np.array([0.0]*m).reshape(1, m), shape=[1, m], dtype=tf.float32)\n","                vec_zero = tf.tile(zero, tf.stack([shape_X_t, 1]))\n","                vec_cost = tf.concat([cost, vec_zero], axis=1)\n","\n","                Vsecond_t = Vprime_t - vec_cost\n","\n","                V_t = tf.multiply(Vsecond_t, y_t)\n","                self.portfolioValue = tf.norm(V_t, ord=1)\n","                self.instantaneous_reward = (self.portfolioValue-self.pf_value_previous)/self.pf_value_previous\n","                \n","            with tf.variable_scope(\"Reward_Equiweighted\"):                      # computation of the reward of equiwighted portfolio\n","                constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n","                cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n","                y_t = tf.concat([cash_return, self.dailyReturn_t], axis=1)\n","  \n","                V_eq = w_eq*self.pf_value_previous\n","                V_eq_second = tf.multiply(V_eq, y_t)\n","        \n","                self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n","            \n","                self.instantaneous_reward_eq = (self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n","                \n","            with tf.variable_scope(\"Max_weight\"):                               # largest change in weight\n","                self.max_weight = tf.reduce_max(self.action)\n","                #print(self.max_weight.shape)\n","                \n","            with tf.variable_scope(\"Reward_adjusted\"):\n","                self.adjusted_reward = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul * self.max_weight\n","                \n","        #objective function\n","        self.train_op = optimizer.minimize(-self.adjusted_reward)               # maximize reward over the batch, where min(-r) = max(r)\n","        self.optimizer = optimizer\n","        self.sess = sess\n","\n","    def compute_W(self, X_t_, W_previous_):\n","        \"\"\"\n","        This function returns the action the agent takes given the input tensor and the W_previous.\n","        It is a vector of weights\n","        \"\"\"\n","        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n","\n","    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n","        \"\"\"\n","        This function trains the neural network\n","        maximizing the reward  and the input is a batch of the differents values\n","        \"\"\"\n","        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n","                                                self.W_previous: W_previous_,\n","                                                self.pf_value_previous: pf_value_previous_,\n","                                                self.dailyReturn_t: dailyReturn_t_})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMu9nqBkioDP","colab_type":"text"},"source":["### Portfolio Vector Memory\n"]},{"cell_type":"code","metadata":{"id":"N2poaWrFitTj","colab_type":"code","colab":{}},"source":["class PVM(object):\n","    \"\"\"\n","    This is the memory stack of weights, called Portfolio Vector Memory\n","    \"\"\"\n","    def __init__(self, m, sample_bias, total_steps = total_steps_train, \n","                 batch_size = batch_size, w_init = w_init_train):\n","        \n","        self.memory = np.transpose(np.array([w_init]*total_steps))              # initialization of the memory  \n","        self.sample_bias = sample_bias\n","        self.total_steps = total_steps                                          # number of times of initialization of portfolio tensor\n","        self.batch_size = batch_size\n","\n","    def get_W(self, t):\n","        \"\"\"\n","        Return the weight from the PVM at time t \n","        \"\"\"\n","        return self.memory[:, t]\n","\n","    def update(self, t, w):\n","        \"\"\"\n","        Update the weight at time t\n","        \"\"\"\n","        self.memory[:, t] = w\n","\n","    def draw(self, beta=sample_bias):\n","        \"\"\"\n","        returns a valid step so you can get a training batch starting at this step\n","        \"\"\"\n","        while 1:\n","            z = np.random.geometric(p=beta)\n","            tb = self.total_steps - self.batch_size + 1 - z\n","            if tb >= 0:\n","                return tb\n","            \n","    def test(self):\n","        \"\"\"\n","        For test purposes\n","        \"\"\"\n","        return self.memory"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piSxArbviuOq","colab_type":"text"},"source":["### Evaluation function"]},{"cell_type":"code","metadata":{"id":"9Ds5Vj1Biyk0","colab_type":"code","colab":{}},"source":["def eval_perf(e):\n","    \"\"\"\n","    This function evaluates the performance of the different types of agents. \n","    \"\"\"\n","    list_weight_end_val = list()\n","    list_pf_end_training = list()\n","    list_pf_min_training = list()\n","    list_pf_max_training = list()\n","    list_pf_mean_training = list()\n","    \n","    #environment for trading of the agent \n","    env_eval = TradeEnv(path=path_data, window_length=n,\n","                        portfolio_value=pf_init_train, trading_cost=trading_cost,\n","                        interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n","\n","    #initialization of the environment \n","    state_eval, done_eval = env_eval.reset(w_init_test, pf_init_test, t = total_steps_train)\n","\n","    #first element of the weight and portfolio value \n","    p_list_eval = [pf_init_test]\n","    w_list_eval = [w_init_test]\n","\n","    for k in range(total_steps_train, total_steps_train +total_steps_val-int(n/2)):\n","        X_t = state_eval[0].reshape([-1]+ list(state_eval[0].shape))\n","        W_previous = state_eval[1].reshape([-1]+ list(state_eval[1].shape))\n","        pf_value_previous = state_eval[2]\n","\n","        action = actor.compute_W(X_t, W_previous)                               # compute the action\n","        \n","        state_eval, reward_eval, done_eval = env_eval.step(action)              # make step forward in environment\n","\n","        X_next = state_eval[0]\n","        W_t_eval = state_eval[1]\n","        pf_value_t_eval = state_eval[2]\n","\n","        dailyReturn_t = X_next[-1, :, -1]\n","        #print('current portfolio value', round(pf_value_previous,0))\n","        #print('weights', W_previous)\n","        p_list_eval.append(pf_value_t_eval)\n","        w_list_eval.append(W_t_eval)\n","        \n","    list_weight_end_val.append(w_list_eval[-1])\n","    list_pf_end_training.append(p_list_eval[-1])\n","    list_pf_min_training.append(np.min(p_list_eval))\n","    list_pf_max_training.append(np.max(p_list_eval))\n","    list_pf_mean_training.append(np.mean(p_list_eval))\n","    \n","    print('End of test PF value:',round(p_list_eval[-1]))\n","    print('Min of test PF value:',round(np.min(p_list_eval)))\n","    print('Max of test PF value:',round(np.max(p_list_eval)))\n","    print('Mean of test PF value:',round(np.mean(p_list_eval)))\n","    print('End of test weights:',w_list_eval[-1])\n","    plt.title('Portfolio evolution (validation set) episode {}'.format(e))\n","    plt.plot(p_list_eval, label = 'Agent Portfolio Value')\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","    plt.show()\n","    plt.title('Portfolio weights (end of validation set) episode {}'.format(e))\n","    plt.bar(np.arange(m+1), list_weight_end_val[-1])\n","    plt.xticks(np.arange(m+1), ['Money'] + list_stock, rotation=45)\n","    plt.show()\n","    \n","    \n","    names = ['Money'] + list_stock\n","    w_list_eval = np.array(w_list_eval)\n","    for j in range(m+1):\n","        plt.plot(w_list_eval[:,j], label = 'Weight Stock {}'.format(names[j]))\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n","    plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xKVIyu09iyL_","colab_type":"text"},"source":["### Envirionment Creation"]},{"cell_type":"code","metadata":{"id":"ku6mZV4LjZY5","colab_type":"code","colab":{}},"source":["# environment for trading of the agent \n","# this is the agent trading environment (policy network agent)\n","env = TradeEnv(path=path_data, window_length=n,\n","               portfolio_value=pf_init_train, trading_cost=trading_cost,\n","               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n","\n","# environment for equiweighted\n","# this environment is set up for an agent who only plays an equiweithed portfolio (baseline)\n","env_eq = TradeEnv(path=path_data, window_length=n,\n","               portfolio_value=pf_init_train, trading_cost=trading_cost,\n","               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n","\n","# environment secured (only money)\n","# this environment is set up for an agentwho plays secure, keeps its money\n","env_s = TradeEnv(path=path_data, window_length=n,\n","               portfolio_value=pf_init_train, trading_cost=trading_cost,\n","               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n","\n","#full on one stock environment \n","#these environments are set up for agents who play only on one stock\n","action_fu = list()\n","env_fu = list()\n","for i in range(m):\n","    action_i = np.array([0]*(i+1) + [1] + [0]*(m-(i+1)))\n","    action_fu.append(action_i)\n","    env_i = TradeEnv(path=path_data, window_length=n,\n","               portfolio_value=pf_init_train, trading_cost=trading_cost,\n","               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n","    env_fu.append(env_i) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VollqVGNYO1k","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"IsnQMqkQYRXU","colab_type":"code","colab":{}},"source":["############# TRAINING #####################\n","###########################################\n","\n","\n","# initialize session\n","tf.reset_default_graph()\n","sess = tf.Session()\n","\n","# initialize networks\n","actor = Policy(m, n, sess, optimizer,                                           # policy initialization\n","                 trading_cost=trading_cost, \n","                 interest_rate=interest_rate)  \n","\n","# initialize tensorflow graphs\n","sess.run(tf.global_variables_initializer())\n","\n","\n","\n","list_final_pf = list()\n","list_final_pf_eq = list()\n","list_final_pf_s = list()\n","\n","list_final_pf_fu = list()\n","state_fu = [0]*m\n","done_fu = [0]*m\n","\n","pf_value_t_fu = [0]*m\n","\n","for i in range(m):\n","    list_final_pf_fu.append(list())\n","    \n","\n","###### Train #####\n","for e in range(n_episodes):\n","    print('Start Episode', e)\n","    if e==0:\n","        eval_perf('Before Training')\n","    print('Episode:', e)\n","    #init the PVM with the training parameters\n","    memory = PVM(m,sample_bias, total_steps = total_steps_train, \n","                 batch_size = batch_size, w_init = w_init_train)\n","    \n","    for nb in range(n_batches):\n","        print('Batch:',nb)\n","        #draw the starting point of the batch \n","        i_start = memory.draw()\n","        \n","        \n","        #reset the environment with the weight from PVM at the starting point\n","        #reset also with a portfolio value with initial portfolio value\n","        state, done = env.reset(memory.get_W(i_start), pf_init_train, t=i_start )\n","        state_eq, done_eq = env_eq.reset(w_eq, pf_init_train, t=i_start )\n","        state_s, done_s = env_s.reset(w_s, pf_init_train, t=i_start )\n","        \n","        for i in range(m):\n","            state_fu[i], done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_train, t=i_start )\n","        \n","        \n","        \n","        list_X_t, list_W_previous, list_pf_value_previous, list_dailyReturn_t = [], [], [], []\n","        list_pf_value_previous_eq, list_pf_value_previous_s = [],[]\n","        list_pf_value_previous_fu = list()\n","        for i in range(m):\n","            list_pf_value_previous_fu.append(list())\n","            \n","        \n","        \n","        \n","        \n","        for bs in range(batch_size):\n","            \n","            #load the different inputs from the previous loaded state \n","            X_t = state[0].reshape([-1] + list(state[0].shape))\n","            W_previous = state[1].reshape([-1] + list(state[1].shape))\n","            pf_value_previous = state[2]\n","            \n","            \n","            if np.random.rand()< ratio_greedy:\n","                #print('go')\n","                #computation of the action of the agent\n","                action = actor.compute_W(X_t, W_previous)\n","            else:\n","                action = get_random_action(m)\n","            \n","            #given the state and the action, call the environment to go one time step later \n","            state, reward, done = env.step(action)\n","            state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n","            state_s, reward_s, done_s = env_s.step(w_s)\n","            \n","            for i in range(m):\n","                state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n","\n","            \n","            \n","            #get the new state \n","            X_next = state[0]\n","            W_t = state[1]\n","            pf_value_t = state[2]\n","            \n","            pf_value_t_eq = state_eq[2]\n","            pf_value_t_s = state_s[2]\n","            \n","            for i in range(m):\n","                pf_value_t_fu[i] = state_fu[i][2]\n","                \n","            \n","            #let us compute the returns \n","            dailyReturn_t = X_next[-1, :, -1]\n","            #update into the PVM\n","            memory.update(i_start+bs, W_t)\n","            #store elements\n","            list_X_t.append(X_t.reshape(state[0].shape))\n","            list_W_previous.append(W_previous.reshape(state[1].shape))\n","            list_pf_value_previous.append([pf_value_previous])\n","            list_dailyReturn_t.append(dailyReturn_t)\n","            \n","            list_pf_value_previous_eq.append(pf_value_t_eq)\n","            list_pf_value_previous_s.append(pf_value_t_s)\n","            \n","            for i in range(m):\n","                list_pf_value_previous_fu[i].append(pf_value_t_fu[i])\n","            \n","            if bs==batch_size-1:\n","                list_final_pf.append(pf_value_t)\n","                list_final_pf_eq.append(pf_value_t_eq)\n","                list_final_pf_s.append(pf_value_t_s)\n","                for i in range(m):\n","                    list_final_pf_fu[i].append(pf_value_t_fu[i])\n","            \n","            #printing\n","            if bs==0:\n","                print('start', i_start)\n","                print('PF_start', round(pf_value_previous,0))\n","\n","            if bs==batch_size-1:\n","                print('PF_end', round(pf_value_t,0))\n","                print('weight', W_t)\n","\n","        list_X_t = np.array(list_X_t)\n","        list_W_previous = np.array(list_W_previous)\n","        list_pf_value_previous = np.array(list_pf_value_previous)\n","        list_dailyReturn_t = np.array(list_dailyReturn_t)\n","        \n","        \n","        #for each batch, train the network to maximize the reward\n","        actor.train(list_X_t, list_W_previous,\n","                    list_pf_value_previous, list_dailyReturn_t)\n","    eval_perf(e)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pPv5g5mYQZn","colab_type":"text"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"DDiY8wDbYUAk","colab_type":"code","colab":{}},"source":["#######TEST#######\n","\n","#initialization of the environment \n","state, done = env.reset(w_init_test, pf_init_test, t = total_steps_train)\n","\n","state_eq, done_eq = env_eq.reset(w_eq, pf_init_test, t = total_steps_train)\n","state_s, done_s = env_s.reset(w_s, pf_init_test, t = total_steps_train)\n","\n","for i in range(m):\n","    state_fu[i],  done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_test, t = total_steps_train)\n","\n","\n","#first element of the weight and portfolio value \n","p_list = [pf_init_test]\n","w_list = [w_init_test]\n","\n","p_list_eq = [pf_init_test]\n","p_list_s = [pf_init_test]\n","\n","\n","p_list_fu = list()\n","for i in range(m):\n","    p_list_fu.append([pf_init_test])\n","    \n","pf_value_t_fu = [0]*m\n","    \n","\n","for k in range(total_steps_train +total_steps_val-int(n/2), total_steps_train +total_steps_val +total_steps_test -n):\n","    X_t = state[0].reshape([-1]+ list(state[0].shape))\n","    W_previous = state[1].reshape([-1]+ list(state[1].shape))\n","    pf_value_previous = state[2]\n","    #compute the action \n","    action = actor.compute_W(X_t, W_previous)\n","    #step forward environment \n","    state, reward, done = env.step(action)\n","    state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n","    state_s, reward_s, done_s = env_s.step(w_s)\n","    \n","    \n","    for i in range(m):\n","        state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n","    \n","    \n","    X_next = state[0]\n","    W_t = state[1]\n","    pf_value_t = state[2]\n","    \n","    pf_value_t_eq = state_eq[2]\n","    pf_value_t_s = state_s[2]\n","    for i in range(m):\n","        pf_value_t_fu[i] = state_fu[i][2]\n","    \n","    dailyReturn_t = X_next[-1, :, -1]\n","    if k%20 == 0:\n","        print('current portfolio value', round(pf_value_previous,0))\n","        print('weights', W_previous)\n","    p_list.append(pf_value_t)\n","    w_list.append(W_t)\n","    \n","    p_list_eq.append(pf_value_t_eq)\n","    p_list_s.append(pf_value_t_s)\n","    for i in range(m):\n","        p_list_fu[i].append(pf_value_t_fu[i])\n","        \n","    #here to breack the loop/not in original code     \n","    if k== total_steps_train +total_steps_val-int(n/2) + 100:\n","        break\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vs5Saw6NYhnU","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"ObjZnyRqYiGQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}