{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the main part of the project. It is decomposed in the following parts:\n",
    "- Parameters setting \n",
    "- Creation of the trading environment \n",
    "- Set-up of the trading agent (actor)\n",
    "- Set-up of the portfolio vector memory (PVM)\n",
    "- Agent training \n",
    "- Agent Evaluation\n",
    "- Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Note:</u> This notebook has been cleaned up and run on a local machine. The appearing results are only for illustration and not representative of the project results in the presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRBAcvyDeOBF"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "import ffn\n",
    "from environment import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU0NVhPpd4y6"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './input.npy'\n",
    "data_dir = 'StocksD'\n",
    "data_type = data_dir\n",
    "stocks_tickers = ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO'] \n",
    "list_stock = stocks_tickers\n",
    "data = np.load(path_data)\n",
    "trading_period = data.shape[2]\n",
    "nb_feature_map = data.shape[0]\n",
    "nb_stocks = data.shape[1]\n",
    "m = nb_stocks\n",
    "f = nb_feature_map\n",
    "t = trading_period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################dictionaries of the problem###########################\n",
    "dict_hp_net = {'n_filter_1': 2, 'n_filter_2': 20, 'kernel1_size':(1, 3)}\n",
    "dict_hp_pb = {'batch_size': 50, 'ratio_train': 0.6,'ratio_val': 0.2, 'length_tensor': 10,\n",
    "              'ratio_greedy':0.8, 'ratio_regul': 0.1}\n",
    "dict_hp_opt = {'regularization': 1e-8, 'learning': 9e-2}\n",
    "dict_fin = {'trading_cost': 0.25/100, 'interest_rate': 0.02/250, 'cash_bias_init': 0.7}\n",
    "dict_train = {'pf_init_train': 10000, 'w_init_train': 'd', 'n_episodes':2, 'n_batches':10}\n",
    "dict_test = {'pf_init_test': 10000, 'w_init_test': 'd'}\n",
    "###############################HP of the network ###########################\n",
    "n_filter_1 = dict_hp_net['n_filter_1'] # 2\n",
    "n_filter_2 = dict_hp_net['n_filter_2'] # 20\n",
    "kernel1_size = dict_hp_net['kernel1_size'] #(1,3)\n",
    "###############################HP of the problem###########################\n",
    "batch_size = dict_hp_pb['batch_size']                               # Size of mini-batch during training\n",
    "total_steps_train = int(dict_hp_pb['ratio_train']*trading_period)   # Total number of steps for pre-training in the training set\n",
    "total_steps_val = int(dict_hp_pb['ratio_val']*trading_period)       # Total number of steps for pre-training in the validation set\n",
    "total_steps_test = trading_period-total_steps_train-total_steps_val # Total number of steps for the test\n",
    "n = dict_hp_pb['length_tensor']                                     # Num of columns (number of the trading periods) in each input price matrix\n",
    "ratio_greedy = dict_hp_pb['ratio_greedy']\n",
    "ratio_regul = dict_hp_pb['ratio_regul']\n",
    "##############################HP of the optimization###########################\n",
    "regularization = dict_hp_opt['regularization']                      # The L2 regularization coefficient applied to network training\n",
    "learning = dict_hp_opt['learning']                                  # Parameter alpha (i.e. the step size) of the Adam optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning)\n",
    "##############################Finance parameters###########################\n",
    "trading_cost= dict_fin['trading_cost']\n",
    "interest_rate= dict_fin['interest_rate']\n",
    "cash_bias_init = dict_fin['cash_bias_init']\n",
    "sample_bias = 5e-5                                                  # Beta in the geometric distribution for online training sample batches\n",
    "############################## Training Parameters ###########################\n",
    "w_init_train = np.array(np.array([1]+[0]*m))    #dict_train['w_init_train']\n",
    "pf_init_train = dict_train['pf_init_train']\n",
    "n_episodes = dict_train['n_episodes']\n",
    "n_batches = dict_train['n_batches']\n",
    "############################## Test Parameters ###########################\n",
    "w_init_test = np.array(np.array([1]+[0]*m))     #dict_test['w_init_test']\n",
    "pf_init_test = dict_test['pf_init_test']\n",
    "############################## other environment Parameters ###########################\n",
    "w_eq = np.array(np.array([1/(m+1)]*(m+1)))\n",
    "w_s = np.array(np.array([1]+[0.0]*m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random action function\n",
    "def get_random_action(m):\n",
    "    random_vec = np.random.rand(m+1)\n",
    "    return random_vec/np.sum(random_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.16251442, 0.19153284, 0.09130694, 0.13205896, 0.22054849,\n       0.20203834])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_action(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment for trading of the agent \n",
    "# this is the agent trading environment (policy network agent)\n",
    "env = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "# environment for equiweighted\n",
    "# this environment is set up for an agent who only plays an equiweithed portfolio (baseline)\n",
    "env_eq = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "# environment secured (only money)\n",
    "# this environment is set up for an agentwho plays secure, keeps its money\n",
    "env_s = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full on one stock environment \n",
    "#these environments are set up for agents who play only on one stock\n",
    "\n",
    "action_fu = list()\n",
    "env_fu = list()\n",
    "\n",
    "for i in range(m):\n",
    "    action = np.array([0]*(i+1) + [1] + [0]*(m-(i+1)))\n",
    "    action_fu.append(action)\n",
    "    \n",
    "    env_fu_i = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "    \n",
    "    env_fu.append(env_fu_i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net \\pi_\\phi(s) as a class\n",
    "class Policy(object):\n",
    "    '''\n",
    "    This class is used to instanciate the policy network agent\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, m, n, sess, optimizer,\n",
    "                 trading_cost=trading_cost,\n",
    "                 interest_rate=interest_rate,\n",
    "                 n_filter_1=n_filter_1,\n",
    "                 n_filter_2=n_filter_2):\n",
    "\n",
    "        # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.n_filter_1 = n_filter_1\n",
    "        self.n_filter_2 = n_filter_2\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "\n",
    "        with tf.variable_scope(\"Inputs\"):\n",
    "            # Placeholder\n",
    "            self.X_t = tf.placeholder(\n",
    "                tf.float32, [None, nb_feature_map, self.m, self.n])         # The Price tensor\n",
    "            self.W_previous = tf.placeholder(tf.float32, [None, self.m+1])  # weights at the previous time step\n",
    "            self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])  # portfolio value at the previous time step\n",
    "            self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m]) # vector of Open(t+1)/Open(t)\n",
    "            #self.pf_value_previous_eq = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        with tf.variable_scope(\"Policy_Model\"):\n",
    "\n",
    "            bias = tf.get_variable('cash_bias', shape=[1, 1, 1, 1], \n",
    "                                   initializer=tf.constant_initializer(cash_bias_init))  # variable of the cash bias\n",
    "            shape_X_t = tf.shape(self.X_t)[0]                               # shape of the tensor == batchsize\n",
    "            self.cash_bias = tf.tile(bias, tf.stack([shape_X_t, 1, 1, 1]))  # trick to get a \"tensor size\" for the cash bias\n",
    "            # print(self.cash_bias.shape)\n",
    "\n",
    "            with tf.variable_scope(\"Conv1\"):\n",
    "                # first layer on the X_t tensor, return a tensor of depth 2\n",
    "                self.conv1 = tf.layers.conv2d(\n",
    "                    inputs=tf.transpose(self.X_t, perm=[0, 3, 2, 1]),\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_1,\n",
    "                    strides=(1, 1),\n",
    "                    kernel_size=kernel1_size,\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Conv2\"):\n",
    "                # feature maps\n",
    "                self.conv2 = tf.layers.conv2d(\n",
    "                    inputs=self.conv1,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_2,\n",
    "                    strides=(self.n, 1),\n",
    "                    kernel_size=(1, self.n),\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor3\"):\n",
    "                # w from last periods, modify to have good dimensions\n",
    "                w_wo_c = self.W_previous[:, 1:]\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, 1)\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, -1)\n",
    "                self.tensor3 = tf.concat([self.conv2, w_wo_c], axis=3)\n",
    "\n",
    "            with tf.variable_scope(\"Conv3\"):\n",
    "                # last feature map WITHOUT cash bias\n",
    "                self.conv3 = tf.layers.conv2d(\n",
    "                    inputs=self.conv2,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=1,\n",
    "                    strides=(self.n_filter_2 + 1, 1),\n",
    "                    kernel_size=(1, 1),\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor4\"):\n",
    "                self.tensor4 = tf.concat([self.cash_bias, self.conv3], axis=2)  # last feature map WITH cash bias\n",
    "                self.squeezed_tensor4 = tf.squeeze(self.tensor4, [1, 3])        # we squeeze to reduce and get the good dimension\n",
    "\n",
    "            with tf.variable_scope(\"Policy_Output\"):\n",
    "                self.action = tf.nn.softmax(self.squeezed_tensor4)              # softmax layer to obtain weights\n",
    "\n",
    "            with tf.variable_scope(\"Reward\"):\n",
    "                # computation of the reward, (please look at the chronological map to understand)\n",
    "                constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat([cash_return, self.dailyReturn_t], axis=1)\n",
    "                Vprime_t = self.action * self.pf_value_previous\n",
    "                Vprevious = self.W_previous*self.pf_value_previous\n",
    "\n",
    "                constant = tf.constant(1.0, shape=[1])                          # modify to get the good shape for cost\n",
    "                cost = self.trading_cost * tf.norm(Vprime_t-Vprevious, ord=1, axis=1)*constant\n",
    "                cost = tf.expand_dims(cost, 1)\n",
    "                zero = tf.constant(np.array([0.0]*m).reshape(1, m), shape=[1, m], dtype=tf.float32)\n",
    "                vec_zero = tf.tile(zero, tf.stack([shape_X_t, 1]))\n",
    "                vec_cost = tf.concat([cost, vec_zero], axis=1)\n",
    "\n",
    "                Vsecond_t = Vprime_t - vec_cost\n",
    "\n",
    "                V_t = tf.multiply(Vsecond_t, y_t)\n",
    "                self.portfolioValue = tf.norm(V_t, ord=1)\n",
    "                self.instantaneous_reward = (self.portfolioValue-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_Equiweighted\"):\n",
    "                constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat(;[cash_return, self.dailyReturn_t], axis=1)\n",
    "  \n",
    "                V_eq = w_eq*self.pf_value_previous\n",
    "                V_eq_second = tf.multiply(V_eq, y_t)\n",
    "        \n",
    "                self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "            \n",
    "                self.instantaneous_reward_eq = (\n",
    "                    self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "            with tf.variable_scope(\"Max_weight\"):\n",
    "                self.max_weight = tf.reduce_max(self.action)\n",
    "                print(self.max_weight.shape)\n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_adjusted\"):\n",
    "                self.adjested_reward = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul*self.max_weight\n",
    "                \n",
    "        #objective function - maximize reward over the batch \n",
    "        # min(-r) = max(r)\n",
    "        self.train_op = optimizer.minimize(-self.adjested_reward)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "\n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        \"\"\"\n",
    "        This function returns the action the agent takes \n",
    "        given the input tensor and the W_previous\n",
    "        \n",
    "        It is a vector of weight\n",
    "\n",
    "        \"\"\"\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "\n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        \"\"\"\n",
    "        This function trains the neural network\n",
    "        maximizing the reward \n",
    "        the input is a batch of the differents values\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the PVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVM(object):\n",
    "    '''\n",
    "    This is the memory stack called PVM in the paper\n",
    "    '''\n",
    "\n",
    "    def __init__(self, m, sample_bias, total_steps = total_steps_train, \n",
    "                 batch_size = batch_size, w_init = w_init_train):\n",
    "        \n",
    "        \n",
    "        #initialization of the memory \n",
    "        #we have a total_step_times the initialization portfolio tensor \n",
    "        self.memory = np.transpose(np.array([w_init]*total_steps))  \n",
    "        self.sample_bias = sample_bias\n",
    "        self.total_steps = total_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_W(self, t):\n",
    "        #return the weight from the PVM at time t \n",
    "        return self.memory[:, t]\n",
    "\n",
    "    def update(self, t, w):\n",
    "        #update the weight at time t\n",
    "        self.memory[:, t] = w\n",
    "\n",
    "\n",
    "    def draw(self, beta=sample_bias):\n",
    "        '''\n",
    "        returns a valid step so you can get a training batch starting at this step\n",
    "        '''\n",
    "        while 1:\n",
    "            z = np.random.geometric(p=beta)\n",
    "            tb = self.total_steps - self.batch_size + 1 - z\n",
    "            if tb >= 0:\n",
    "                return tb\n",
    "            \n",
    "    def test(self):\n",
    "        #just to test\n",
    "        return self.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUT4gLhveOBW"
   },
   "source": [
    "Try to rollout trajecories using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_draw_down(xs):\n",
    "    xs = np.array(xs)\n",
    "    i = np.argmax(np.maximum.accumulate(xs) - xs) # end of the period\n",
    "    j = np.argmax(xs[:i]) # start of period\n",
    "    \n",
    "    return xs[j] - xs[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perf(e):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of the different types of agents. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    list_weight_end_val = list()\n",
    "    list_pf_end_training = list()\n",
    "    list_pf_min_training = list()\n",
    "    list_pf_max_training = list()\n",
    "    list_pf_mean_training = list()\n",
    "    list_pf_dd_training = list()\n",
    "    \n",
    "    #######TEST#######\n",
    "    #environment for trading of the agent \n",
    "    env_eval = TradeEnv(path=path_data, window_length=n,\n",
    "                   portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "                   interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "\n",
    "\n",
    "    #initialization of the environment \n",
    "    state_eval, done_eval = env_eval.reset(w_init_test, pf_init_test, t = total_steps_train)\n",
    "\n",
    "\n",
    "\n",
    "    #first element of the weight and portfolio value \n",
    "    p_list_eval = [pf_init_test]\n",
    "    w_list_eval = [w_init_test]\n",
    "\n",
    "    for k in range(total_steps_train, total_steps_train +total_steps_val-int(n/2)):\n",
    "        X_t = state_eval[0].reshape([-1]+ list(state_eval[0].shape))\n",
    "        W_previous = state_eval[1].reshape([-1]+ list(state_eval[1].shape))\n",
    "        pf_value_previous = state_eval[2]\n",
    "        #compute the action \n",
    "        action = actor.compute_W(X_t, W_previous)\n",
    "        #step forward environment \n",
    "        state_eval, reward_eval, done_eval = env_eval.step(action)\n",
    "\n",
    "        X_next = state_eval[0]\n",
    "        W_t_eval = state_eval[1]\n",
    "        pf_value_t_eval = state_eval[2]\n",
    "\n",
    "        dailyReturn_t = X_next[-1, :, -1]\n",
    "        #print('current portfolio value', round(pf_value_previous,0))\n",
    "        #print('weights', W_previous)\n",
    "        p_list_eval.append(pf_value_t_eval)\n",
    "        w_list_eval.append(W_t_eval)\n",
    "        \n",
    "    list_weight_end_val.append(w_list_eval[-1])\n",
    "    list_pf_end_training.append(p_list_eval[-1])\n",
    "    list_pf_min_training.append(np.min(p_list_eval))\n",
    "    list_pf_max_training.append(np.max(p_list_eval))\n",
    "    list_pf_mean_training.append(np.mean(p_list_eval))\n",
    "    \n",
    "    list_pf_dd_training.append(get_max_draw_down(p_list_eval))\n",
    "\n",
    "    print('End of test PF value:',round(p_list_eval[-1]))\n",
    "    print('Min of test PF value:',round(np.min(p_list_eval)))\n",
    "    print('Max of test PF value:',round(np.max(p_list_eval)))\n",
    "    print('Mean of test PF value:',round(np.mean(p_list_eval)))\n",
    "    print('Max Draw Down of test PF value:',round(get_max_draw_down(p_list_eval)))\n",
    "    print('End of test weights:',w_list_eval[-1])\n",
    "    plt.title('Portfolio evolution (validation set) episode {}'.format(e))\n",
    "    plt.plot(p_list_eval, label = 'Agent Portfolio Value')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    plt.title('Portfolio weights (end of validation set) episode {}'.format(e))\n",
    "    plt.bar(np.arange(m+1), list_weight_end_val[-1])\n",
    "    plt.xticks(np.arange(m+1), ['Money'] + list_stock, rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    names = ['Money'] + list_stock\n",
    "    w_list_eval = np.array(w_list_eval)\n",
    "    for j in range(m+1):\n",
    "        plt.plot(w_list_eval[:,j], label = 'Weight Stock {}'.format(names[j]))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "()\nStart Episode 0\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b76985e31e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Episode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0meval_perf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#init the PVM with the training parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ed428159fa9c>\u001b[0m in \u001b[0;36meval_perf\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpf_value_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#compute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_W\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_previous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m#step forward environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstate_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e4d1e3284dc4>\u001b[0m in \u001b[0;36mcompute_W\u001b[0;34m(self, X_t_, W_previous_)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_t_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_previous\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mW_previous_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_t_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_previous_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf_value_previous_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdailyReturn_t_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/anaconda3/envs/drl_env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############# TRAINING #####################\n",
    "###########################################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize networks\n",
    "actor = Policy(m, n, sess, optimizer,\n",
    "                 trading_cost=trading_cost, \n",
    "                 interest_rate=interest_rate)  # policy initialization\n",
    "\n",
    "# initialize tensorflow graphs\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "list_final_pf = list()\n",
    "list_final_pf_eq = list()\n",
    "list_final_pf_s = list()\n",
    "\n",
    "list_final_pf_fu = list()\n",
    "state_fu = [0]*m\n",
    "done_fu = [0]*m\n",
    "\n",
    "pf_value_t_fu = [0]*m\n",
    "\n",
    "for i in range(m):\n",
    "    list_final_pf_fu.append(list())\n",
    "    \n",
    "\n",
    "###### Train #####\n",
    "for e in range(n_episodes):\n",
    "    print('Start Episode', e)\n",
    "    #if e==0:\n",
    "    #    eval_perf('Before Training')\n",
    "    print('Episode:', e)\n",
    "    #init the PVM with the training parameters\n",
    "    memory = PVM(m,sample_bias, total_steps = total_steps_train, \n",
    "                 batch_size = batch_size, w_init = w_init_train)\n",
    "    \n",
    "    for nb in range(n_batches):\n",
    "        print('Batch:',nb)\n",
    "        #draw the starting point of the batch \n",
    "        i_start = memory.draw()\n",
    "        \n",
    "        \n",
    "        #reset the environment with the weight from PVM at the starting point\n",
    "        #reset also with a portfolio value with initial portfolio value\n",
    "        state, done = env.reset(memory.get_W(i_start), pf_init_train, t=i_start )\n",
    "        state_eq, done_eq = env_eq.reset(w_eq, pf_init_train, t=i_start )\n",
    "        state_s, done_s = env_s.reset(w_s, pf_init_train, t=i_start )\n",
    "        \n",
    "        for i in range(m):\n",
    "            state_fu[i], done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_train, t=i_start )\n",
    "        \n",
    "        \n",
    "        \n",
    "        list_X_t, list_W_previous, list_pf_value_previous, list_dailyReturn_t = [], [], [], []\n",
    "        list_pf_value_previous_eq, list_pf_value_previous_s = [],[]\n",
    "        list_pf_value_previous_fu = list()\n",
    "        for i in range(m):\n",
    "            list_pf_value_previous_fu.append(list())\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for bs in range(batch_size):\n",
    "            \n",
    "            #load the different inputs from the previous loaded state \n",
    "            X_t = state[0].reshape([-1] + list(state[0].shape))\n",
    "            W_previous = state[1].reshape([-1] + list(state[1].shape))\n",
    "            pf_value_previous = state[2]\n",
    "            \n",
    "            \n",
    "            if np.random.rand()< ratio_greedy:\n",
    "                #print('go')\n",
    "                #computation of the action of the agent\n",
    "                action = actor.compute_W(X_t, W_previous)\n",
    "            else:\n",
    "                action = get_random_action(m)\n",
    "            \n",
    "            #given the state and the action, call the environment to go one time step later \n",
    "            state, reward, done = env.step(action)\n",
    "            state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n",
    "            state_s, reward_s, done_s = env_s.step(w_s)\n",
    "            \n",
    "            for i in range(m):\n",
    "                state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n",
    "\n",
    "            \n",
    "            \n",
    "            #get the new state \n",
    "            X_next = state[0]\n",
    "            W_t = state[1]\n",
    "            pf_value_t = state[2]\n",
    "            \n",
    "            pf_value_t_eq = state_eq[2]\n",
    "            pf_value_t_s = state_s[2]\n",
    "            \n",
    "            for i in range(m):\n",
    "                pf_value_t_fu[i] = state_fu[i][2]\n",
    "                \n",
    "            \n",
    "            #let us compute the returns \n",
    "            dailyReturn_t = X_next[-1, :, -1]\n",
    "            #update into the PVM\n",
    "            memory.update(i_start+bs, W_t)\n",
    "            #store elements\n",
    "            list_X_t.append(X_t.reshape(state[0].shape))\n",
    "            list_W_previous.append(W_previous.reshape(state[1].shape))\n",
    "            list_pf_value_previous.append([pf_value_previous])\n",
    "            list_dailyReturn_t.append(dailyReturn_t)\n",
    "            \n",
    "            list_pf_value_previous_eq.append(pf_value_t_eq)\n",
    "            list_pf_value_previous_s.append(pf_value_t_s)\n",
    "            \n",
    "            for i in range(m):\n",
    "                list_pf_value_previous_fu[i].append(pf_value_t_fu[i])\n",
    "            \n",
    "            if bs==batch_size-1:\n",
    "                list_final_pf.append(pf_value_t)\n",
    "                list_final_pf_eq.append(pf_value_t_eq)\n",
    "                list_final_pf_s.append(pf_value_t_s)\n",
    "                for i in range(m):\n",
    "                    list_final_pf_fu[i].append(pf_value_t_fu[i])\n",
    "\n",
    "            \n",
    "            \n",
    "#             #printing\n",
    "#             if bs==0:\n",
    "#                 print('start', i_start)\n",
    "#                 print('PF_start', round(pf_value_previous,0))\n",
    "\n",
    "#             if bs==batch_size-1:\n",
    "#                 print('PF_end', round(pf_value_t,0))\n",
    "#                 print('weight', W_t)\n",
    "\n",
    "        list_X_t = np.array(list_X_t)\n",
    "        list_W_previous = np.array(list_W_previous)\n",
    "        list_pf_value_previous = np.array(list_pf_value_previous)\n",
    "        list_dailyReturn_t = np.array(list_dailyReturn_t)\n",
    "        \n",
    "        \n",
    "        #for each batch, train the network to maximize the reward\n",
    "        actor.train(list_X_t, list_W_previous,\n",
    "                    list_pf_value_previous, list_dailyReturn_t)\n",
    "    eval_perf(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######TEST#######\n",
    "\n",
    "\n",
    "#initialization of the environment \n",
    "state, done = env.reset(w_init_test, pf_init_test, t = total_steps_train)\n",
    "\n",
    "state_eq, done_eq = env_eq.reset(w_eq, pf_init_test, t = total_steps_train)\n",
    "state_s, done_s = env_s.reset(w_s, pf_init_test, t = total_steps_train)\n",
    "\n",
    "for i in range(m):\n",
    "    state_fu[i],  done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_test, t = total_steps_train)\n",
    "\n",
    "\n",
    "#first element of the weight and portfolio value \n",
    "p_list = [pf_init_test]\n",
    "w_list = [w_init_test]\n",
    "\n",
    "p_list_eq = [pf_init_test]\n",
    "p_list_s = [pf_init_test]\n",
    "\n",
    "\n",
    "p_list_fu = list()\n",
    "for i in range(m):\n",
    "    p_list_fu.append([pf_init_test])\n",
    "    \n",
    "pf_value_t_fu = [0]*m\n",
    "    \n",
    "\n",
    "for k in range(total_steps_train +total_steps_val-int(n/2), total_steps_train +total_steps_val +total_steps_test -n):\n",
    "    X_t = state[0].reshape([-1]+ list(state[0].shape))\n",
    "    W_previous = state[1].reshape([-1]+ list(state[1].shape))\n",
    "    pf_value_previous = state[2]\n",
    "    #compute the action \n",
    "    action = actor.compute_W(X_t, W_previous)\n",
    "    #step forward environment \n",
    "    state, reward, done = env.step(action)\n",
    "    state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n",
    "    state_s, reward_s, done_s = env_s.step(w_s)\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n",
    "    \n",
    "    \n",
    "    X_next = state[0]\n",
    "    W_t = state[1]\n",
    "    pf_value_t = state[2]\n",
    "    \n",
    "    pf_value_t_eq = state_eq[2]\n",
    "    pf_value_t_s = state_s[2]\n",
    "    for i in range(m):\n",
    "        pf_value_t_fu[i] = state_fu[i][2]\n",
    "    \n",
    "    dailyReturn_t = X_next[-1, :, -1]\n",
    "    if k%20 == 0:\n",
    "        print('current portfolio value', round(pf_value_previous,0))\n",
    "        print('weights', W_previous)\n",
    "    p_list.append(pf_value_t)\n",
    "    w_list.append(W_t)\n",
    "    \n",
    "    p_list_eq.append(pf_value_t_eq)\n",
    "    p_list_s.append(pf_value_t_s)\n",
    "    for i in range(m):\n",
    "        p_list_fu[i].append(pf_value_t_fu[i])\n",
    "        \n",
    "    #here to breack the loop/not in original code     \n",
    "    if k== total_steps_train +total_steps_val-int(n/2) + 100:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"individual_stocks_5yr/\"\n",
    "times = pd.read_csv(path+\"A_data.csv\").date\n",
    "test_start_day =total_steps_train +total_steps_val-int(n/2)+10\n",
    "times = list(times[test_start_day:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size, learning, ratio_greedy, e, n, kernel1_size, n_batches, ratio_regul\n",
    "\n",
    "data_type = path_data.split('/')[2][5:].split('.')[0]\n",
    "namesBio=['JNJ','PFE','AMGN','MDT','CELG','LLY']\n",
    "namesUtilities=['XOM','CVX','MRK','SLB','MMM']\n",
    "namesTech=['FB','AMZN','MSFT','AAPL','T','VZ','CMCSA','IBM','CRM','INTC']\n",
    "\n",
    "\n",
    "if data_type == 'Utilities':\n",
    "    list_stock = namesUtilities\n",
    "elif data_type == 'Bio':\n",
    "    list_stock = namesBio\n",
    "elif data_type == 'Tech':\n",
    "    list_stock = namesTech\n",
    "else:\n",
    "    list_stock = [i for i in range(m)]\n",
    "\n",
    "\n",
    "plt.title('Portfolio Value (Test Set) {}: {}, {}, {}, {}, {}, {}, {}, {}'.format(data_type, batch_size, learning, ratio_greedy, e, n, kernel1_size, n_batches, ratio_regul))\n",
    "plt.plot(p_list, label = 'Agent Portfolio Value')\n",
    "plt.plot(p_list_eq, label = 'Equi-weighted Portfolio Value')\n",
    "plt.plot(p_list_s, label = 'Secured Portfolio Value')\n",
    "for i in range(m):\n",
    "    plt.plot(p_list_fu[i], label = 'Full Stock {} Portfolio Value'.format(list_stock[i]))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Money'] + list_stock\n",
    "w_list = np.array(w_list)\n",
    "for j in range(m+1):\n",
    "    plt.plot(w_list[:,j], label = 'Weight Stock {}'.format(names[j]))\n",
    "    plt.title('Weight evolution during testing')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(p_list)-np.array(p_list_eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1=0\n",
    "index2=-1\n",
    "\n",
    "plt.plot(list_final_pf[index1:index2], label = 'Agent Portfolio Value')\n",
    "plt.plot(list_final_pf_eq[index1:index2], label = 'Baseline Portfolio Value')\n",
    "plt.plot(list_final_pf_s[index1:index2], label = 'Secured Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((np.array(list_final_pf)-np.array(list_final_pf_eq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T18:54:39.717210Z",
     "start_time": "2018-05-05T18:54:39.712361Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DPM_v2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "696px",
    "left": "611.992px",
    "right": "20px",
    "top": "113.984px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}